{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0f735d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5f8d603d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import cv2\n",
    "import numpy as np\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49700b3c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e7f6e46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "60a9a50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DDPM():\n",
    "    def __init__(self,\n",
    "                 device,\n",
    "                 n_steps: int,\n",
    "                 min_beta: float = 0.0001,\n",
    "                 max_beta: float = 0.02):\n",
    "        betas = torch.linspace(min_beta, max_beta, n_steps).to(device)\n",
    "        alphas = 1 - betas\n",
    "        alpha_bars = torch.empty_like(alphas)\n",
    "        product = 1\n",
    "        for i, alpha in enumerate(alphas):\n",
    "            product *= alpha\n",
    "            alpha_bars[i] = product\n",
    "        self.betas = betas\n",
    "        self.n_steps = n_steps\n",
    "        self.alphas = alphas\n",
    "        self.alpha_bars = alpha_bars\n",
    "    def sample_forward(self, x, t, eps=None):\n",
    "        alpha_bar = self.alpha_bars[t].reshape(-1, 1, 1, 1)\n",
    "        if eps is None:\n",
    "            eps = torch.randn_like(x)\n",
    "        res = eps * torch.sqrt(1 - alpha_bar) + torch.sqrt(alpha_bar) * x\n",
    "        return res\n",
    "    def sample_backward_step(self, x_t, t, net, simple_var=True):\n",
    "        n = x_t.shape[0]\n",
    "        t_tensor = torch.tensor([t] * n,\n",
    "                                dtype=torch.long).to(x_t.device).unsqueeze(1)\n",
    "        eps, _ = net(x_t, t_tensor)\n",
    "\n",
    "        if t == 0:\n",
    "            noise = 0\n",
    "        else:\n",
    "            if simple_var:\n",
    "                var = self.betas[t]\n",
    "            else:\n",
    "                var = (1 - self.alpha_bars[t - 1]) / (\n",
    "                    1 - self.alpha_bars[t]) * self.betas[t]\n",
    "            noise = torch.randn_like(x_t)\n",
    "            noise *= torch.sqrt(var)\n",
    "\n",
    "        mean = (x_t -\n",
    "                (1 - self.alphas[t]) / torch.sqrt(1 - self.alpha_bars[t]) *\n",
    "                eps) / torch.sqrt(self.alphas[t])\n",
    "        x_t = mean + noise\n",
    "\n",
    "        return x_t\n",
    "    def sample_backward(self, img_shape, net, device, simple_var=True):\n",
    "        x = torch.randn(img_shape).to(device)\n",
    "        net = net.to(device)\n",
    "        for t in range(self.n_steps-1, -1, -1):\n",
    "            x = self.sample_backward_step(x, t, net, simple_var)\n",
    "        return x\n",
    "    def training_sample_backward_with_t(self, img_shape, net, device, t_end, simple_var=True):\n",
    "        x = torch.randn(img_shape).to(device)\n",
    "        net = net.to(device)\n",
    "        for t in range(self.n_steps-1, t_end-1, -1):\n",
    "            x = self.sample_backward_step(x, t, net, simple_var)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb0f70a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Classifier(nn.Module):\n",
    "    def __init__(self, output_size):\n",
    "        super(Classifier, self).__init__()\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(16, 32, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(64 * 4 * 4, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_size)  # Output has 10 classes\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ee05ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloader(batch_size: int):\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor()    ])\n",
    "    train_dataset = torchvision.datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "    \n",
    "    target_label = 0\n",
    "    filtered_indices = [i for i, label in enumerate(train_dataset.targets) if label == target_label]\n",
    "    dataset = torch.utils.data.Subset(train_dataset, filtered_indices)\n",
    "    return DataLoader(dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b8a60c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_shape():\n",
    "    return 1, 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2d7f46",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8844e722",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d627b85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "\n",
    "    def __init__(self, max_seq_len: int, d_model: int):\n",
    "        super().__init__()\n",
    "\n",
    "        # Assume d_model is an even number for convenience\n",
    "        assert d_model % 2 == 0\n",
    "\n",
    "        pe = torch.zeros(max_seq_len, d_model)\n",
    "        i_seq = torch.linspace(0, max_seq_len - 1, max_seq_len)\n",
    "        j_seq = torch.linspace(0, d_model - 2, d_model // 2)\n",
    "        pos, two_i = torch.meshgrid(i_seq, j_seq, indexing='ij')\n",
    "        pe_2i = torch.sin(pos / 10000**(two_i / d_model))\n",
    "        pe_2i_1 = torch.cos(pos / 10000**(two_i / d_model))\n",
    "        pe = torch.stack((pe_2i, pe_2i_1), 2).reshape(max_seq_len, d_model)\n",
    "\n",
    "        self.embedding = nn.Embedding(max_seq_len, d_model)\n",
    "        self.embedding.weight.data = pe\n",
    "        self.embedding.requires_grad_(False)\n",
    "\n",
    "    def forward(self, t):\n",
    "        return self.embedding(t)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_c: int, out_c: int):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, 1, 1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_c)\n",
    "        self.actvation1 = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_c)\n",
    "        self.actvation2 = nn.ReLU()\n",
    "        if in_c != out_c:\n",
    "            self.shortcut = nn.Sequential(nn.Conv2d(in_c, out_c, 1),\n",
    "                                          nn.BatchNorm2d(out_c))\n",
    "        else:\n",
    "            self.shortcut = nn.Identity()\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = self.bn1(x)\n",
    "        x = self.actvation1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x += self.shortcut(input)\n",
    "        x = self.actvation2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class ConvNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_steps,\n",
    "                 intermediate_channels=[10, 20, 40],\n",
    "                 pe_dim=10,\n",
    "                 insert_t_to_all_layers=False):\n",
    "        super().__init__()\n",
    "        C, H, W = get_img_shape()  # 1, 28, 28\n",
    "        self.pe = PositionalEncoding(n_steps, pe_dim)\n",
    "\n",
    "        self.pe_linears = nn.ModuleList()\n",
    "        self.all_t = insert_t_to_all_layers\n",
    "        if not insert_t_to_all_layers:\n",
    "            self.pe_linears.append(nn.Linear(pe_dim, C))\n",
    "\n",
    "        self.residual_blocks = nn.ModuleList()\n",
    "        prev_channel = C\n",
    "        for channel in intermediate_channels:\n",
    "            self.residual_blocks.append(ResidualBlock(prev_channel, channel))\n",
    "            if insert_t_to_all_layers:\n",
    "                self.pe_linears.append(nn.Linear(pe_dim, prev_channel))\n",
    "            else:\n",
    "                self.pe_linears.append(None)\n",
    "            prev_channel = channel\n",
    "        self.output_layer = nn.Conv2d(prev_channel, C, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        n = t.shape[0]\n",
    "        t = self.pe(t)\n",
    "        for m_x, m_t in zip(self.residual_blocks, self.pe_linears):\n",
    "            if m_t is not None:\n",
    "                pe = m_t(t).reshape(n, -1, 1, 1)\n",
    "                x = x + pe\n",
    "            x = m_x(x)\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, shape, in_c, out_c, residual=False):\n",
    "        super().__init__()\n",
    "        self.ln = nn.LayerNorm(shape)\n",
    "        self.conv1 = nn.Conv2d(in_c, out_c, 3, 1, 1)\n",
    "        self.conv2 = nn.Conv2d(out_c, out_c, 3, 1, 1)\n",
    "        self.activation = nn.ReLU()\n",
    "        self.residual = residual\n",
    "        if residual:\n",
    "            if in_c == out_c:\n",
    "                self.residual_conv = nn.Identity()\n",
    "            else:\n",
    "                self.residual_conv = nn.Conv2d(in_c, out_c, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.ln(x)\n",
    "        out = self.conv1(out)\n",
    "        out = self.activation(out)\n",
    "        out = self.conv2(out)\n",
    "        if self.residual:\n",
    "            out += self.residual_conv(x)\n",
    "        out = self.activation(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 n_steps,\n",
    "                 channels=[10, 20, 40, 80],\n",
    "                 pe_dim=10,\n",
    "                 residual=False) -> None:\n",
    "        super().__init__()\n",
    "        C, H, W = get_img_shape()\n",
    "        layers = len(channels)\n",
    "        Hs = [H]\n",
    "        Ws = [W]\n",
    "        cH = H\n",
    "        cW = W\n",
    "        for _ in range(layers - 1):\n",
    "            cH //= 2\n",
    "            cW //= 2\n",
    "            Hs.append(cH)\n",
    "            Ws.append(cW)\n",
    "\n",
    "        self.pe = PositionalEncoding(n_steps, pe_dim)\n",
    "\n",
    "        self.encoders = nn.ModuleList()\n",
    "        self.decoders = nn.ModuleList()\n",
    "        self.pe_linears_en = nn.ModuleList()\n",
    "        self.pe_linears_de = nn.ModuleList()\n",
    "        self.downs = nn.ModuleList()\n",
    "        self.ups = nn.ModuleList()\n",
    "        prev_channel = C\n",
    "        for channel, cH, cW in zip(channels[0:-1], Hs[0:-1], Ws[0:-1]):\n",
    "            self.pe_linears_en.append(\n",
    "                nn.Sequential(nn.Linear(pe_dim, prev_channel), nn.ReLU(),\n",
    "                              nn.Linear(prev_channel, prev_channel)))\n",
    "            self.encoders.append(\n",
    "                nn.Sequential(\n",
    "                    UnetBlock((prev_channel, cH, cW),\n",
    "                              prev_channel,\n",
    "                              channel,\n",
    "                              residual=residual),\n",
    "                    UnetBlock((channel, cH, cW),\n",
    "                              channel,\n",
    "                              channel,\n",
    "                              residual=residual)))\n",
    "            self.downs.append(nn.Conv2d(channel, channel, 2, 2))\n",
    "            prev_channel = channel\n",
    "\n",
    "        self.pe_mid = nn.Linear(pe_dim, prev_channel)\n",
    "        channel = channels[-1]\n",
    "        self.mid = nn.Sequential(\n",
    "            UnetBlock((prev_channel, Hs[-1], Ws[-1]),\n",
    "                      prev_channel,\n",
    "                      channel,\n",
    "                      residual=residual),\n",
    "            UnetBlock((channel, Hs[-1], Ws[-1]),\n",
    "                      channel,\n",
    "                      channel,\n",
    "                      residual=residual),\n",
    "        )\n",
    "        prev_channel = channel\n",
    "        for channel, cH, cW in zip(channels[-2::-1], Hs[-2::-1], Ws[-2::-1]):\n",
    "            self.pe_linears_de.append(nn.Linear(pe_dim, prev_channel))\n",
    "            self.ups.append(nn.ConvTranspose2d(prev_channel, channel, 2, 2))\n",
    "            self.decoders.append(\n",
    "                nn.Sequential(\n",
    "                    UnetBlock((channel * 2, cH, cW),\n",
    "                              channel * 2,\n",
    "                              channel,\n",
    "                              residual=residual),\n",
    "                    UnetBlock((channel, cH, cW),\n",
    "                              channel,\n",
    "                              channel,\n",
    "                              residual=residual)))\n",
    "\n",
    "            prev_channel = channel\n",
    "\n",
    "        self.conv_out = nn.Conv2d(prev_channel, C, 3, 1, 1)\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        n = t.shape[0]\n",
    "        t = self.pe(t)\n",
    "        encoder_outs = []\n",
    "        for pe_linear, encoder, down in zip(self.pe_linears_en, self.encoders,\n",
    "                                            self.downs):\n",
    "            pe = pe_linear(t).reshape(n, -1, 1, 1)\n",
    "            x = encoder(x + pe)\n",
    "            encoder_outs.append(x)\n",
    "            x = down(x)\n",
    "        pe = self.pe_mid(t).reshape(n, -1, 1, 1)\n",
    "        x = self.mid(x + pe)\n",
    "        \n",
    "        mid_x = []\n",
    "        for pe_linear, decoder, up, encoder_out in zip(self.pe_linears_de,\n",
    "                                                       self.decoders, self.ups,\n",
    "                                                       encoder_outs[::-1]):\n",
    "            pe = pe_linear(t).reshape(n, -1, 1, 1)\n",
    "            x = up(x)\n",
    "            \n",
    "            pad_x = encoder_out.shape[2] - x.shape[2]\n",
    "            pad_y = encoder_out.shape[3] - x.shape[3]\n",
    "            x = F.pad(x, (pad_x // 2, pad_x - pad_x // 2, pad_y // 2,\n",
    "                          pad_y - pad_y // 2))\n",
    "            x = torch.cat((encoder_out, x), dim=1)\n",
    "            x = decoder(x + pe)\n",
    "\n",
    "            mid_x.append(x)\n",
    "        x = self.conv_out(x)\n",
    "        return x, mid_x\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def build_network(config: dict, n_steps):\n",
    "    network_type = config.pop('type')\n",
    "    if network_type == 'ConvNet':\n",
    "        network_cls = ConvNet\n",
    "    elif network_type == 'UNet':\n",
    "        network_cls = UNet\n",
    "\n",
    "    network = network_cls(n_steps, **config)\n",
    "    return network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf45ba56",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, channels, n_size):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.channels = channels\n",
    "        self.n_size = n_size\n",
    "\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(channels, channels*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Conv2d(channels*2, channels*4, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        mid_layer_size = int(np.sqrt(channels*4*n_size*(n_size+1)))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(channels*4*n_size*(n_size+1), mid_layer_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(mid_layer_size, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x, t):\n",
    "        x = torch.cat((x, t.unsqueeze(1).unsqueeze(1).unsqueeze(1).repeat(1, self.channels, self.n_size, 1)), dim=3)\n",
    "        x = self.features(x)\n",
    "        \n",
    "        x = self.classifier(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779cdb03",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "302d4dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce02928c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(ddpm: DDPM, net, device):\n",
    "    n_steps = ddpm.n_steps\n",
    "    dataloader = get_dataloader(batch_size)\n",
    "    net = net.to(device)\n",
    "    #loss_fn = nn.MSELoss()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=0.001)\n",
    "    discriminator_1 = Discriminator(16, 3).to(device) # (-1, 16, 3, 3)\n",
    "    discriminator_1_optimizer = torch.optim.Adam(discriminator_1.parameters(), lr=0.001)\n",
    "    \n",
    "    discriminator_2 = Discriminator(8, 7).to(device)\n",
    "    discriminator_2_optimizer = torch.optim.Adam(discriminator_2.parameters(), lr=0.001)\n",
    "    \n",
    "    discriminator_3 = Discriminator(4, 14).to(device)\n",
    "    discriminator_3_optimizer = torch.optim.Adam(discriminator_3.parameters(), lr=0.001)\n",
    "    \n",
    "    discriminator_4 = Discriminator(2, 28).to(device)\n",
    "    discriminator_4_optimizer = torch.optim.Adam(discriminator_4.parameters(), lr=0.001)\n",
    "    for e in range(n_epochs):\n",
    "        tot_loss = 0\n",
    "        count_tot = 0\n",
    "        for x, _ in tqdm(dataloader):\n",
    "            current_batch_size = x.shape[0]\n",
    "            x = x.to(device)\n",
    "            t = torch.randint(0, n_steps, (current_batch_size, )).to(device)\n",
    "            eps = torch.randn_like(x).to(device)\n",
    "            x_t = ddpm.sample_forward(x, t, eps)\n",
    "            eps_theta, x_mid = net(x_t, t.reshape(current_batch_size, 1))\n",
    "            \n",
    "            #loss = loss_fn(eps_theta, eps)\n",
    "            #optimizer.zero_grad()\n",
    "            #loss.backward(retain_graph=True)\n",
    "            #optimizer.step()\n",
    "            \n",
    "            # Generate true data   \n",
    "            input_t = t/1000\n",
    "            real_labels = torch.ones(batch_size, 1).to(device)\n",
    "            real_outputs = discriminator_1(x_mid[0], input_t)\n",
    "            loss_real_1 = criterion(real_outputs, real_labels)\n",
    "            \n",
    "            real_outputs = discriminator_2(x_mid[1], input_t)\n",
    "            loss_real_2 = criterion(real_outputs, real_labels)\n",
    "            \n",
    "            real_outputs = discriminator_3(x_mid[2], input_t)\n",
    "            loss_real_3 = criterion(real_outputs, real_labels)\n",
    "            \n",
    "            real_outputs = discriminator_4(x_mid[3], input_t)\n",
    "            loss_real_4 = criterion(real_outputs, real_labels)\n",
    "            \n",
    "            # Generate fake data\n",
    "            net.eval()\n",
    "            x_t_fake = torch.zeros_like(x_t)\n",
    "            count = 0\n",
    "            for end_time in t:\n",
    "                fake_img = ddpm.training_sample_backward_with_t((1, *get_img_shape()), net, device=device, t_end=end_time, simple_var=True)\n",
    "                x_t_fake[count] = fake_img[0]\n",
    "                count += 1\n",
    "            eps_theta, x_mid_fake = net(x_t_fake, t.reshape(current_batch_size, 1))\n",
    "            \n",
    "            \n",
    "            fake_labels = torch.zeros(batch_size, 1).to(device)\n",
    "            fake_outputs = discriminator_1(x_mid_fake[0], input_t)\n",
    "            loss_fake_1 = criterion(fake_outputs, fake_labels)\n",
    "            \n",
    "            fake_outputs = discriminator_2(x_mid_fake[1], input_t)\n",
    "            loss_fake_2 = criterion(fake_outputs, fake_labels)\n",
    "            \n",
    "            fake_outputs = discriminator_3(x_mid_fake[2], input_t)\n",
    "            loss_fake_3 = criterion(fake_outputs, fake_labels)\n",
    "            \n",
    "            fake_outputs = discriminator_4(x_mid_fake[3], input_t)\n",
    "            loss_fake_4 = criterion(fake_outputs, fake_labels)\n",
    "            \n",
    "            \n",
    "\n",
    "            discriminator_1_optimizer.zero_grad()\n",
    "            loss_discriminator_1 = loss_real_1 + loss_fake_1\n",
    "            loss_discriminator_1.backward(retain_graph=True)\n",
    "            discriminator_1_optimizer.step()\n",
    "            \n",
    "            discriminator_2_optimizer.zero_grad()\n",
    "            loss_discriminator_2 = loss_real_2 + loss_fake_2\n",
    "            loss_discriminator_2.backward(retain_graph=True)\n",
    "            discriminator_2_optimizer.step()\n",
    "            \n",
    "            discriminator_3_optimizer.zero_grad()\n",
    "            loss_discriminator_3 = loss_real_3 + loss_fake_3\n",
    "            loss_discriminator_3.backward(retain_graph=True)\n",
    "            discriminator_3_optimizer.step()\n",
    "            \n",
    "            discriminator_4_optimizer.zero_grad()\n",
    "            loss_discriminator_4 = loss_real_4 + loss_fake_4\n",
    "            loss_discriminator_4.backward(retain_graph=True)\n",
    "            discriminator_4_optimizer.step()\n",
    "            net.train()\n",
    "            \n",
    "            # Train Generator\n",
    "            discriminator_1.eval()\n",
    "            discriminator_2.eval()\n",
    "            discriminator_3.eval()\n",
    "            discriminator_4.eval()\n",
    "            discriminator_outputs = discriminator_1(x_mid_fake[0], input_t) # can generate again but waste a lot of time\n",
    "            loss_generator_1 = criterion(discriminator_outputs, real_labels)\n",
    "            \n",
    "            discriminator_outputs = discriminator_2(x_mid_fake[1], input_t) # can generate again but waste a lot of time\n",
    "            loss_generator_2 = criterion(discriminator_outputs, real_labels)\n",
    "            \n",
    "            discriminator_outputs = discriminator_3(x_mid_fake[2], input_t) # can generate again but waste a lot of time\n",
    "            loss_generator_3 = criterion(discriminator_outputs, real_labels)\n",
    "            \n",
    "            discriminator_outputs = discriminator_4(x_mid_fake[3], input_t) # can generate again but waste a lot of time\n",
    "            loss_generator_4 = criterion(discriminator_outputs, real_labels)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss_generator = loss_generator_1 + loss_generator_2 + loss_generator_3 + loss_generator_4\n",
    "            loss_generator.backward()\n",
    "            optimizer.step()\n",
    "            discriminator_1.train()\n",
    "            discriminator_2.train()\n",
    "            discriminator_3.train()\n",
    "            discriminator_4.train()\n",
    "\n",
    "\n",
    "            \n",
    "            #tot_loss += loss\n",
    "            count_tot += 1\n",
    "            print(loss_discriminator_1.item(), loss_discriminator_2.item(), loss_discriminator_3.item(), \n",
    "                  loss_discriminator_4.item(), loss_generator.item())\n",
    " \n",
    "            \n",
    "        \n",
    "        print(tot_loss/count, e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0f55ea44",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_steps = 1000\n",
    "device = 'cuda'\n",
    "unet_res_cfg = {\n",
    "    'type': 'UNet',\n",
    "    'channels': [2, 4, 8, 16, 32], # [32, 64, 128, 256, 512],\n",
    "    'pe_dim': 128,\n",
    "    'residual': True\n",
    "}\n",
    "\n",
    "config = unet_res_cfg \n",
    "net = build_network(config, n_steps)\n",
    "ddpm = DDPM(device, n_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e07b4d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 4\n",
    "n_epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b897a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                               | 1/1481 [00:17<7:10:18, 17.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.387046217918396 1.3895437717437744 1.4026113748550415 1.39162278175354 2.9908976554870605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                              | 2/1481 [01:02<13:45:55, 33.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3841091394424438 1.3808860778808594 1.2926363945007324 1.3948407173156738 3.4814951419830322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                             | 3/1481 [01:35<13:39:57, 33.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3843748569488525 1.3748176097869873 1.2708134651184082 1.3784431219100952 3.4021143913269043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▏                                                                             | 4/1481 [01:57<11:54:06, 29.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3843436241149902 1.3745739459991455 1.2961554527282715 1.3811421394348145 3.298703670501709\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                              | 5/1481 [02:13<9:55:03, 24.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3857375383377075 1.38468599319458 1.3335182666778564 1.371068000793457 3.066570281982422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                             | 6/1481 [03:22<16:07:48, 39.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3773170709609985 1.3108714818954468 0.7838331460952759 1.3030471801757812 6.855902194976807\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|▎                                                                             | 7/1481 [03:58<15:42:46, 38.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3845305442810059 1.3431065082550049 1.1140636205673218 1.3328635692596436 4.272899150848389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                                                             | 8/1481 [04:15<12:54:45, 31.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3857300281524658 1.3811562061309814 1.2312195301055908 1.3633005619049072 3.095360279083252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▍                                                                             | 9/1481 [05:18<16:57:24, 41.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3822288513183594 1.2758911848068237 0.7743897438049316 1.246812343597412 7.880884170532227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                            | 10/1481 [06:15<18:49:37, 46.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.380469560623169 1.2830945253372192 0.6708828210830688 1.2564499378204346 7.735708236694336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                            | 11/1481 [06:33<15:24:39, 37.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3841361999511719 1.3765428066253662 1.2470581531524658 1.3039886951446533 2.968637466430664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▌                                                                            | 12/1481 [07:19<16:21:28, 40.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3860936164855957 1.3171595335006714 0.8707162737846375 1.221889615058899 5.759948253631592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                            | 13/1481 [08:04<16:59:00, 41.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3788141012191772 1.24483323097229 0.7384771108627319 1.2316007614135742 8.236825942993164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▋                                                                            | 14/1481 [08:19<13:41:43, 33.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3873049020767212 1.3883490562438965 1.3828407526016235 1.2526042461395264 2.793604850769043\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                            | 15/1481 [09:12<16:00:37, 39.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.384290099143982 1.251955270767212 0.7094318866729736 1.1394566297531128 10.080446243286133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▊                                                                            | 16/1481 [09:56<16:35:02, 40.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.384049415588379 1.2819205522537231 0.9389348030090332 1.052594780921936 8.321377754211426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                            | 17/1481 [10:27<15:27:27, 38.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3837969303131104 1.3227369785308838 0.9389462471008301 1.3101595640182495 7.144023895263672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                            | 18/1481 [10:59<14:36:38, 35.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3853611946105957 1.3808014392852783 0.9680840969085693 1.0970314741134644 3.986727714538574\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|▉                                                                            | 19/1481 [11:27<13:41:44, 33.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.382396936416626 1.3318562507629395 1.0956833362579346 1.493964433670044 6.8798627853393555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  1%|█                                                                            | 20/1481 [12:08<14:32:35, 35.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.385057806968689 1.368796467781067 0.8696962594985962 1.192336082458496 5.737338066101074\n"
     ]
    }
   ],
   "source": [
    "train(ddpm, net, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed2e259a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5985936e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb2f4a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "bc31699c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(1.8450)"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cross_entropy(input, target):\n",
    "    return torch.mean(-torch.sum(target * torch.log(input), 1))\n",
    "\n",
    "\n",
    "y = torch.Tensor([[1, 1, 1, 1]])\n",
    "yhat = torch.Tensor([[0.3502, 1.4223, 0.9054, 0.3504]])\n",
    "cross_entropy(yhat, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d8eeef8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95d7737a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99b1308",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b9847a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57148dcb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e0647af",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "f0fbfa33",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), \"model_unet_res_01.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "12b044f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"model_unet_res_01.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "829c2f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "inv_normalize = transforms.Normalize(\n",
    "    mean=[-0.4914/0.2463, -0.4821/0.2428, -0.4465/0.2607],\n",
    "    std=[1/0.2463, 1/0.2428, 1/0.2607]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2a979cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_imgs(ddpm,\n",
    "                net,\n",
    "                output_path,\n",
    "                n_sample=81,\n",
    "                device='cuda',\n",
    "                simple_var=True):\n",
    "    net = net.to(device)\n",
    "    net = net.eval()\n",
    "    with torch.no_grad():\n",
    "        shape = (n_sample, *get_img_shape())  # 1, 3, 28, 28\n",
    "        imgs = ddpm.sample_backward(shape,\n",
    "                                    net,\n",
    "                                    device=device,\n",
    "                                    simple_var=simple_var).detach().cpu()\n",
    "        imgs = inv_normalize(imgs)\n",
    "        imgs = imgs.clamp(0, 255)\n",
    "        imgs = einops.rearrange(imgs,\n",
    "                                '(b1 b2) c h w -> (b1 h) (b2 w) c',\n",
    "                                b1=int(n_sample**0.5))\n",
    "\n",
    "        imgs = imgs.numpy().astype(np.uint8)\n",
    "\n",
    "        cv2.imwrite(output_path, imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "4a4a114b",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_imgs(ddpm,\n",
    "                net,\n",
    "                \"img.png\",\n",
    "                n_sample=81,\n",
    "                device='cuda',\n",
    "                simple_var=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "09d53299",
   "metadata": {},
   "outputs": [],
   "source": [
    "shape = (1, *get_img_shape())  # 10, 1, 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8b8f5f43",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "imgs = ddpm.sample_backward(shape,\n",
    "                                    net,\n",
    "                                    device=device,\n",
    "                                    simple_var=True).detach().cpu()\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n",
    "plt.imshow(imgs.view(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96cc817",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
